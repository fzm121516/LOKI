<h2 align="center">LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Mnultimodal Models<h5 align="center">



## &#x1F389; News



## &#x1F525; Takeaways
<p class="text">
            â€¢ <strong>Diverse modalities:</strong> Our dataset includes high-quality multimodal data generated by recent
            popular synthetic models, covering 
            <span style="color:#ffb60dde; font-weight: bold;">video</span>, 
            <span style="color:rgba(83, 164, 251, 1); font-weight: bold;">image</span>, 
            <span style="color:rgba(41, 208, 108, 1); font-weight: bold;">3D</span>, 
            <span style="color:rgb(166, 72, 255); font-weight: bold;">text</span>,
            <span style="color:rgb(255, 58, 58); font-weight: bold;">audio</span>, while ensuring
            paired real samples from the same domain. <br>
            â€¢ <strong>Heterogeneous category:</strong> Our collected dataset includes <strong>26</strong> detailed categories across different modalities, such as professional <img style="height: 35px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/satellite.png">statellite images, 
            <img style="height: 35px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/medical.png">medical images, <img style="height: 35px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/philosophy.png">philosophical and
            <img style="height: 35px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/ancient-literature.png">ancient chinese text, as well as <span style="color:rgb(255, 58, 58); font-weight: bold;">audio</span> data like 
            <img style="height: 35px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/singing.png">singing voice and <img style="height: 35px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/music.png">music. <br>
            â€¢ <strong>Multi-level tasks:</strong> Our benchmark includes basic true/false labels for tasks such as <img style="height: 30px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/judgement.png"> judgment
            and <img style="height: 30px;" src="https://github.com/opendatalab/LOKI/blob/27f9fa838ee344798e210ee00fa70ab1b32ef6ae/static/img/icons/selection.png"> multiple-choice questions. Additionally, it features fine-grained abnormality annotations for explanation-based questions, aimed at exploring the capabilities of large multimodal models (LMMs) in Transparent Defake detection. <br>
            â€¢ <strong>Multimodal synthetic data evaluation framework:</strong> We propose a comprehensive multimodal
            evaluation framework that supports the input of various data formats, including <span style="color:#ffb60dde; font-weight: bold;">video</span>, 
            <span style="color:rgba(83, 164, 251, 1); font-weight: bold;">image</span>, 
            <span style="color:rgb(166, 72, 255); font-weight: bold;">text</span>, 
            <span style="color:rgb(255, 58, 58); font-weight: bold;">audio</span>, and 
            <span style="color:rgba(116, 116, 116, 0.765); font-weight: bold;">point cloud</span>, into mainstream multimodal models.
</p>

## Contents

- [ðŸŽ‰ News](#-news)
- [ðŸ”¥ Takeaways](#-takeaways)
- [Contents](#contents)
